{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba5fd0d5ce71498695d985fc4b8dbaf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a0d3c31f42d04e77921f3fd98b731d65",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d1fa421a7a40403489f697ef6310740f",
              "IPY_MODEL_6cb43070497e4908b584c147667189af"
            ]
          }
        },
        "a0d3c31f42d04e77921f3fd98b731d65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1fa421a7a40403489f697ef6310740f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af1d9eb50d844c9dabd7c94c38c41970",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3f15a68b6304d1e88868f6fc53925e0"
          }
        },
        "6cb43070497e4908b584c147667189af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4562bf01a9d8459f8e41b990e9e3d550",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:07&lt;00:00, 23674587.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9d30793c6fb47e789ac9f27d4cc0c41"
          }
        },
        "af1d9eb50d844c9dabd7c94c38c41970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3f15a68b6304d1e88868f6fc53925e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4562bf01a9d8459f8e41b990e9e3d550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9d30793c6fb47e789ac9f27d4cc0c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0FYvmoB4ctK"
      },
      "source": [
        "## Import module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zofQPwle4Xw2"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torchsummary import summary\r\n",
        "\r\n",
        "from math import ceil\r\n",
        "import pdb\r\n",
        "import tqdm\r\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o1On3R_4XT0"
      },
      "source": [
        "## EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzoaVUoe37Ov"
      },
      "source": [
        "base_model = [\r\n",
        "    # expand_ratio, channels, repeats, stride, kernel_size\r\n",
        "    [1, 16, 1, 1, 3],\r\n",
        "    [6, 24, 2, 2, 3],\r\n",
        "    [6, 40, 2, 2, 5],\r\n",
        "    [6, 80, 3, 2, 3],\r\n",
        "    [6, 112, 3, 1, 5],\r\n",
        "    [6, 192, 4, 2, 5],\r\n",
        "    [6, 320, 1, 1, 3],\r\n",
        "]\r\n",
        "\r\n",
        "phi_values = {\r\n",
        "    # tuple of: (phi_value, resolution, drop_rate)\r\n",
        "    'b0': (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\r\n",
        "    'b1': (0.5, 240, 0.2),\r\n",
        "    'b2': (1, 260, 0.3),\r\n",
        "    'b3': (2, 300, 0.3),\r\n",
        "    'b4': (3, 380, 0.4),\r\n",
        "    'b5': (4, 456, 0.4),\r\n",
        "    'b6': (5, 528, 0.5),\r\n",
        "    'b7': (6, 600, 0.5),\r\n",
        "}\r\n",
        "\r\n",
        "class CNNBlock(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):\r\n",
        "        super(CNNBlock, self).__init__()\r\n",
        "        self.cnn = nn.Conv2d(\r\n",
        "            in_channels,\r\n",
        "            out_channels,\r\n",
        "            kernel_size,\r\n",
        "            stride,\r\n",
        "            padding,\r\n",
        "            groups=groups,  # groups=1이면 일반적인 Conv, groups=in_channels 일때만 Depthwise Conv 수행\r\n",
        "            bias=False\r\n",
        "        )\r\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\r\n",
        "        self.silu = nn.SiLU()  # SiLU <-> Swish\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.silu(self.bn(self.cnn(x)))\r\n",
        "\r\n",
        "class SqueezeExcitation(nn.Module):\r\n",
        "    def __init__(self, in_channels, reduced_dim):\r\n",
        "        super(SqueezeExcitation, self).__init__()\r\n",
        "        self.se = nn.Sequential(\r\n",
        "            nn.AdaptiveAvgPool2d(1),  # C x H x W -> C x 1 x 1\r\n",
        "            nn.Conv2d(in_channels, reduced_dim, 1),\r\n",
        "            nn.SiLU(),\r\n",
        "            nn.Conv2d(reduced_dim, in_channels, 1),\r\n",
        "            nn.Sigmoid(),  # 각 채널에 대한 score (0~1)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # pdb.set_trace()\r\n",
        "        return x * self.se(x)  # input channel x 채널의 중요도\r\n",
        "\r\n",
        "\r\n",
        "class InvertedResidualBlock(nn.Module):\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            in_channels,\r\n",
        "            out_channels,\r\n",
        "            kernel_size,\r\n",
        "            stride,\r\n",
        "            padding,\r\n",
        "            expand_ratio,\r\n",
        "            reduction=4,  # squeeze excitation\r\n",
        "            survival_prob=0.8,  # for stochastic depth\r\n",
        "    ):\r\n",
        "        super(InvertedResidualBlock, self).__init__()\r\n",
        "        self.survival_prob = survival_prob\r\n",
        "        self.use_residual = in_channels == out_channels and stride == 1\r\n",
        "        hidden_dim = in_channels * expand_ratio\r\n",
        "        self.expand = in_channels != hidden_dim\r\n",
        "        reduced_dim = int(in_channels / reduction)\r\n",
        "\r\n",
        "        if self.expand:\r\n",
        "            self.expand_conv = CNNBlock(\r\n",
        "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\r\n",
        "            )\r\n",
        "\r\n",
        "        self.conv = nn.Sequential(\r\n",
        "            CNNBlock(\r\n",
        "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,  # Depthwise Conv\r\n",
        "            ),\r\n",
        "            SqueezeExcitation(hidden_dim, reduced_dim),\r\n",
        "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),  # point wise conv\r\n",
        "            nn.BatchNorm2d(out_channels),\r\n",
        "        )\r\n",
        "\r\n",
        "    def stochastic_depth(self, x):\r\n",
        "        '''\r\n",
        "        vanishing gradient로 인해 학습이 느리게 되는 문제를 완화시키고자 stochastic depth 라는 randomness에 기반한 학습 방법\r\n",
        "        Stochastic depth란 network의 depth를 학습 단계에 random하게 줄이는 것을 의미\r\n",
        "        복잡하고 큰 데이터 셋에서는 별다를 효과를 보지는 못한다고 함\r\n",
        "        '''\r\n",
        "        if not self.training:\r\n",
        "            return x\r\n",
        "\r\n",
        "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\r\n",
        "        return torch.div(x, self.survival_prob) * binary_tensor  # torch.div으로 감싼 연산은 stochastic_depth 논문에 나와있음.\r\n",
        "\r\n",
        "    def forward(self, inputs):\r\n",
        "        x = self.expand_conv(inputs) if self.expand else inputs\r\n",
        "\r\n",
        "        if self.use_residual:\r\n",
        "            return self.stochastic_depth(self.conv(x)) + inputs\r\n",
        "        else:\r\n",
        "            return self.conv(x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class EfficientNet(nn.Module):\r\n",
        "    def __init__(self, version, num_classes):\r\n",
        "        super(EfficientNet, self).__init__()\r\n",
        "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\r\n",
        "        last_channels = ceil(1280 * width_factor)\r\n",
        "        self.features = self.create_features(width_factor, depth_factor, last_channels)\r\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)  # stage9 pool\r\n",
        "        self.classifier = nn.Sequential(  # stage9 FC\r\n",
        "            nn.Dropout(dropout_rate),\r\n",
        "            nn.Linear(last_channels, num_classes),\r\n",
        "        )\r\n",
        "\r\n",
        "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\r\n",
        "        phi, res, drop_rate = phi_values[version]\r\n",
        "        depth_factor = alpha ** phi\r\n",
        "        width_factor = beta ** phi\r\n",
        "        return width_factor, depth_factor, drop_rate\r\n",
        "\r\n",
        "    def create_features(self, width_factor, depth_factor, last_channels):\r\n",
        "        channels = int(32 * width_factor)  # B0의 32는 첫레이어의 channel\r\n",
        "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]  # stage 1\r\n",
        "        in_channels = channels\r\n",
        "\r\n",
        "        for expand_ratio, chanels, repeats, stride, kernel_size in base_model:\r\n",
        "            out_channels = 4 * ceil(int(channels*width_factor) / 4)\r\n",
        "            layers_repeats = ceil(repeats * depth_factor)\r\n",
        "\r\n",
        "            for layer in range(layers_repeats):  # stage 2~8\r\n",
        "                features.append(\r\n",
        "                    InvertedResidualBlock(\r\n",
        "                        in_channels,\r\n",
        "                        out_channels,\r\n",
        "                        kernel_size=kernel_size,\r\n",
        "                        stride=stride if layer == 0 else 1,\r\n",
        "                        padding=kernel_size // 2,  # if k=1:pad=0, k=3:pad=1, k=5:pad=2\r\n",
        "                        expand_ratio=expand_ratio\r\n",
        "                    )\r\n",
        "                )\r\n",
        "                in_channels = out_channels\r\n",
        "\r\n",
        "\r\n",
        "        features.append(\r\n",
        "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)  # stage9 Conv 1x1\r\n",
        "        )\r\n",
        "\r\n",
        "        return nn.Sequential(*features)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.pool(self.features(x))\r\n",
        "        return self.classifier(x.view(x.shape[0], -1))  # flatten"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QbvfAjuCyTt"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "ba5fd0d5ce71498695d985fc4b8dbaf0",
            "a0d3c31f42d04e77921f3fd98b731d65",
            "d1fa421a7a40403489f697ef6310740f",
            "6cb43070497e4908b584c147667189af",
            "af1d9eb50d844c9dabd7c94c38c41970",
            "d3f15a68b6304d1e88868f6fc53925e0",
            "4562bf01a9d8459f8e41b990e9e3d550",
            "c9d30793c6fb47e789ac9f27d4cc0c41"
          ]
        },
        "id": "56eIfFQy4BV7",
        "outputId": "037cea81-75aa-44d8-ae4f-5b18fbd97d12"
      },
      "source": [
        "transform_train = transforms.Compose([\r\n",
        "    transforms.RandomCrop(32, padding=4),\r\n",
        "    transforms.RandomHorizontalFlip(),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2434, 0.2615)),\r\n",
        "])\r\n",
        "\r\n",
        "transform_test = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n",
        "])\r\n",
        "\r\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./', train=True, download=True, transform=transform_train)\r\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./', train=False, download=True, transform=transform_test)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba5fd0d5ce71498695d985fc4b8dbaf0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqNU05GYC04l"
      },
      "source": [
        "## Set hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z0jRXsS6MwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0b978d-cebe-4698-b00e-73abe0ecdfc8"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "num_classes = 10\r\n",
        "version = 'b5'\r\n",
        "epochs = 100\r\n",
        "learning_rate = 0.001\r\n",
        "best_acc = 0\r\n",
        "start_epoch = 0\r\n",
        "\r\n",
        "model = EfficientNet(version, num_classes)\r\n",
        "model = model.to(device)\r\n",
        "scaler = torch.cuda.amp.GradScaler()\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\r\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRGXqw9XC5_A"
      },
      "source": [
        "def train(epoch):\r\n",
        "    model.train()\r\n",
        "    train_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    for idx, (inputs, targets) in enumerate(train_loader):\r\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\r\n",
        "\r\n",
        "        # forward\r\n",
        "        with torch.cuda.amp.autocast():    \r\n",
        "            outputs = model(inputs)\r\n",
        "            loss = criterion(outputs, targets)\r\n",
        "        \r\n",
        "        # backward\r\n",
        "        optimizer.zero_grad()\r\n",
        "        scaler.scale(loss).backward()\r\n",
        "        scaler.step(optimizer)\r\n",
        "        scaler.update()\r\n",
        "        \r\n",
        "\r\n",
        "        train_loss += loss.item()\r\n",
        "        _, predicted = outputs.max(1)\r\n",
        "\r\n",
        "        total += targets.size(0)\r\n",
        "        correct += (predicted == targets).sum().item()\r\n",
        "\r\n",
        "        if (idx+1) % 50 == 0:\r\n",
        "            print(f'EPOCH: {epoch+1}/{epochs} idx: {idx+1}/{len(train_loader)} Loss: {loss.item():.3f} accuracy: {predicted.eq(targets).sum().item() / targets.size(0) * 100:.3f}')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYRA6mTGFS7b"
      },
      "source": [
        "def test(epoch):\r\n",
        "    global best_acc\r\n",
        "    model.eval()\r\n",
        "    loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for idx, (inputs, targets) in enumerate(test_loader):\r\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\r\n",
        "\r\n",
        "            outputs = model(inputs)\r\n",
        "\r\n",
        "            loss += criterion(outputs, targets).item()\r\n",
        "            total += targets.size(0)\r\n",
        "\r\n",
        "            _, predicted = outputs.max(1)\r\n",
        "            correct += (predicted == targets).sum().item()\r\n",
        "\r\n",
        "    print(f'Test accuarcy:', 100. * correct / total)\r\n",
        "    print(f'Test average loss:', loss / total)\r\n",
        "\r\n",
        "    acc = correct / total * 100\r\n",
        "    if acc > best_acc:\r\n",
        "        print('Saving')\r\n",
        "        state = {\r\n",
        "            'model': model.state_dict(),\r\n",
        "            'acc': acc,\r\n",
        "            'epoch': epoch,\r\n",
        "        }\r\n",
        "        if not os.path.isdir('checkpoint'):\r\n",
        "            os.mkdir('checkpoint')\r\n",
        "        torch.save(state, 'checkpoint/ckpt.pth')\r\n",
        "        best_acc = acc "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGx5YyhlJ3f_"
      },
      "source": [
        "# fp32 - 194.15641498565674"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RaStFn1G73_",
        "outputId": "b0a8a752-566a-43df-c330-381d60afdf63"
      },
      "source": [
        "for epoch in range(start_epoch, epochs): \r\n",
        "    train(epoch)\r\n",
        "    test(epoch)\r\n",
        "    scheduler.step()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 1/100 idx: 50/391 Loss: 2.278 accuracy: 16.406\n",
            "EPOCH: 1/100 idx: 100/391 Loss: 2.012 accuracy: 23.438\n",
            "EPOCH: 1/100 idx: 150/391 Loss: 1.911 accuracy: 22.656\n",
            "EPOCH: 1/100 idx: 200/391 Loss: 1.957 accuracy: 24.219\n",
            "EPOCH: 1/100 idx: 250/391 Loss: 1.730 accuracy: 27.344\n",
            "EPOCH: 1/100 idx: 300/391 Loss: 1.965 accuracy: 23.438\n",
            "EPOCH: 1/100 idx: 350/391 Loss: 1.905 accuracy: 27.344\n",
            "Test accuarcy: 32.26\n",
            "Test average loss: 0.05307036824226379\n",
            "Saving\n",
            "EPOCH: 2/100 idx: 50/391 Loss: 1.633 accuracy: 42.969\n",
            "EPOCH: 2/100 idx: 100/391 Loss: 1.541 accuracy: 41.406\n",
            "EPOCH: 2/100 idx: 150/391 Loss: 1.630 accuracy: 41.406\n",
            "EPOCH: 2/100 idx: 200/391 Loss: 1.565 accuracy: 38.281\n",
            "EPOCH: 2/100 idx: 250/391 Loss: 1.721 accuracy: 42.969\n",
            "EPOCH: 2/100 idx: 300/391 Loss: 1.228 accuracy: 50.781\n",
            "EPOCH: 2/100 idx: 350/391 Loss: 1.416 accuracy: 43.750\n",
            "Test accuarcy: 49.84\n",
            "Test average loss: 0.042384281659126284\n",
            "Saving\n",
            "EPOCH: 3/100 idx: 50/391 Loss: 1.445 accuracy: 47.656\n",
            "EPOCH: 3/100 idx: 100/391 Loss: 1.464 accuracy: 46.875\n",
            "EPOCH: 3/100 idx: 150/391 Loss: 1.203 accuracy: 54.688\n",
            "EPOCH: 3/100 idx: 200/391 Loss: 1.200 accuracy: 56.250\n",
            "EPOCH: 3/100 idx: 250/391 Loss: 1.270 accuracy: 54.688\n",
            "EPOCH: 3/100 idx: 300/391 Loss: 1.218 accuracy: 50.781\n",
            "EPOCH: 3/100 idx: 350/391 Loss: 1.243 accuracy: 54.688\n",
            "Test accuarcy: 56.97\n",
            "Test average loss: 0.04569738400578499\n",
            "Saving\n",
            "EPOCH: 4/100 idx: 50/391 Loss: 1.050 accuracy: 67.188\n",
            "EPOCH: 4/100 idx: 100/391 Loss: 1.220 accuracy: 55.469\n",
            "EPOCH: 4/100 idx: 150/391 Loss: 1.061 accuracy: 57.812\n",
            "EPOCH: 4/100 idx: 200/391 Loss: 1.095 accuracy: 59.375\n",
            "EPOCH: 4/100 idx: 250/391 Loss: 0.942 accuracy: 64.062\n",
            "EPOCH: 4/100 idx: 300/391 Loss: 1.027 accuracy: 63.281\n",
            "EPOCH: 4/100 idx: 350/391 Loss: 0.891 accuracy: 71.094\n",
            "Test accuarcy: 63.63\n",
            "Test average loss: 0.04311496152877808\n",
            "Saving\n",
            "EPOCH: 5/100 idx: 50/391 Loss: 0.875 accuracy: 71.094\n",
            "EPOCH: 5/100 idx: 100/391 Loss: 1.076 accuracy: 57.812\n",
            "EPOCH: 5/100 idx: 150/391 Loss: 1.181 accuracy: 57.031\n",
            "EPOCH: 5/100 idx: 200/391 Loss: 1.031 accuracy: 66.406\n",
            "EPOCH: 5/100 idx: 250/391 Loss: 0.847 accuracy: 73.438\n",
            "EPOCH: 5/100 idx: 300/391 Loss: 1.250 accuracy: 58.594\n",
            "EPOCH: 5/100 idx: 350/391 Loss: 1.041 accuracy: 61.719\n",
            "Test accuarcy: 66.95\n",
            "Test average loss: 0.1176578627228737\n",
            "Saving\n",
            "EPOCH: 6/100 idx: 50/391 Loss: 0.927 accuracy: 65.625\n",
            "EPOCH: 6/100 idx: 100/391 Loss: 1.039 accuracy: 67.969\n",
            "EPOCH: 6/100 idx: 150/391 Loss: 0.835 accuracy: 67.969\n",
            "EPOCH: 6/100 idx: 200/391 Loss: 0.815 accuracy: 67.969\n",
            "EPOCH: 6/100 idx: 250/391 Loss: 0.797 accuracy: 76.562\n",
            "EPOCH: 6/100 idx: 300/391 Loss: 0.657 accuracy: 80.469\n",
            "EPOCH: 6/100 idx: 350/391 Loss: 0.907 accuracy: 67.969\n",
            "Test accuarcy: 73.21\n",
            "Test average loss: 0.2081989008396864\n",
            "Saving\n",
            "EPOCH: 7/100 idx: 50/391 Loss: 0.649 accuracy: 76.562\n",
            "EPOCH: 7/100 idx: 100/391 Loss: 0.907 accuracy: 64.844\n",
            "EPOCH: 7/100 idx: 150/391 Loss: 0.843 accuracy: 70.312\n",
            "EPOCH: 7/100 idx: 200/391 Loss: 1.029 accuracy: 64.062\n",
            "EPOCH: 7/100 idx: 250/391 Loss: 0.734 accuracy: 73.438\n",
            "EPOCH: 7/100 idx: 300/391 Loss: 0.727 accuracy: 73.438\n",
            "EPOCH: 7/100 idx: 350/391 Loss: 0.882 accuracy: 66.406\n",
            "Test accuarcy: 73.86\n",
            "Test average loss: 0.02944760236442089\n",
            "Saving\n",
            "EPOCH: 8/100 idx: 50/391 Loss: 0.932 accuracy: 65.625\n",
            "EPOCH: 8/100 idx: 100/391 Loss: 0.890 accuracy: 71.875\n",
            "EPOCH: 8/100 idx: 150/391 Loss: 0.843 accuracy: 68.750\n",
            "EPOCH: 8/100 idx: 200/391 Loss: 0.741 accuracy: 78.906\n",
            "EPOCH: 8/100 idx: 250/391 Loss: 0.791 accuracy: 70.312\n",
            "EPOCH: 8/100 idx: 300/391 Loss: 0.747 accuracy: 75.781\n",
            "EPOCH: 8/100 idx: 350/391 Loss: 0.683 accuracy: 73.438\n",
            "Test accuarcy: 73.49\n",
            "Test average loss: 0.21528929880261422\n",
            "EPOCH: 9/100 idx: 50/391 Loss: 0.767 accuracy: 74.219\n",
            "EPOCH: 9/100 idx: 100/391 Loss: 0.739 accuracy: 73.438\n",
            "EPOCH: 9/100 idx: 150/391 Loss: 0.861 accuracy: 69.531\n",
            "EPOCH: 9/100 idx: 200/391 Loss: 0.769 accuracy: 77.344\n",
            "EPOCH: 9/100 idx: 250/391 Loss: 0.722 accuracy: 78.125\n",
            "EPOCH: 9/100 idx: 300/391 Loss: 0.730 accuracy: 75.781\n",
            "EPOCH: 9/100 idx: 350/391 Loss: 0.840 accuracy: 73.438\n",
            "Test accuarcy: 75.2\n",
            "Test average loss: 0.023115704390406607\n",
            "Saving\n",
            "EPOCH: 10/100 idx: 50/391 Loss: 0.778 accuracy: 74.219\n",
            "EPOCH: 10/100 idx: 100/391 Loss: 0.837 accuracy: 75.000\n",
            "EPOCH: 10/100 idx: 150/391 Loss: 0.567 accuracy: 81.250\n",
            "EPOCH: 10/100 idx: 200/391 Loss: 0.583 accuracy: 82.812\n",
            "EPOCH: 10/100 idx: 250/391 Loss: 0.716 accuracy: 74.219\n",
            "EPOCH: 10/100 idx: 300/391 Loss: 0.560 accuracy: 82.031\n",
            "EPOCH: 10/100 idx: 350/391 Loss: 0.809 accuracy: 74.219\n",
            "Test accuarcy: 76.28\n",
            "Test average loss: 0.021247826927900313\n",
            "Saving\n",
            "EPOCH: 11/100 idx: 50/391 Loss: 0.698 accuracy: 74.219\n",
            "EPOCH: 11/100 idx: 100/391 Loss: 0.621 accuracy: 76.562\n",
            "EPOCH: 11/100 idx: 150/391 Loss: 0.710 accuracy: 78.906\n",
            "EPOCH: 11/100 idx: 200/391 Loss: 0.613 accuracy: 77.344\n",
            "EPOCH: 11/100 idx: 250/391 Loss: 0.833 accuracy: 69.531\n",
            "EPOCH: 11/100 idx: 300/391 Loss: 0.595 accuracy: 83.594\n",
            "EPOCH: 11/100 idx: 350/391 Loss: 0.681 accuracy: 77.344\n",
            "Test accuarcy: 78.09\n",
            "Test average loss: 0.020065219350159168\n",
            "Saving\n",
            "EPOCH: 12/100 idx: 50/391 Loss: 0.565 accuracy: 79.688\n",
            "EPOCH: 12/100 idx: 100/391 Loss: 0.680 accuracy: 78.125\n",
            "EPOCH: 12/100 idx: 150/391 Loss: 0.634 accuracy: 81.250\n",
            "EPOCH: 12/100 idx: 200/391 Loss: 0.540 accuracy: 83.594\n",
            "EPOCH: 12/100 idx: 250/391 Loss: 0.600 accuracy: 76.562\n",
            "EPOCH: 12/100 idx: 300/391 Loss: 0.543 accuracy: 80.469\n",
            "EPOCH: 12/100 idx: 350/391 Loss: 0.616 accuracy: 82.031\n",
            "Test accuarcy: 77.2\n",
            "Test average loss: 0.020997286161780357\n",
            "EPOCH: 13/100 idx: 50/391 Loss: 0.517 accuracy: 82.031\n",
            "EPOCH: 13/100 idx: 100/391 Loss: 0.537 accuracy: 81.250\n",
            "EPOCH: 13/100 idx: 150/391 Loss: 0.678 accuracy: 78.125\n",
            "EPOCH: 13/100 idx: 200/391 Loss: 0.558 accuracy: 78.125\n",
            "EPOCH: 13/100 idx: 250/391 Loss: 0.462 accuracy: 85.156\n",
            "EPOCH: 13/100 idx: 300/391 Loss: 0.612 accuracy: 78.906\n",
            "EPOCH: 13/100 idx: 350/391 Loss: 0.526 accuracy: 81.250\n",
            "Test accuarcy: 79.84\n",
            "Test average loss: 0.01864864251613617\n",
            "Saving\n",
            "EPOCH: 14/100 idx: 50/391 Loss: 0.504 accuracy: 79.688\n",
            "EPOCH: 14/100 idx: 100/391 Loss: 0.558 accuracy: 81.250\n",
            "EPOCH: 14/100 idx: 150/391 Loss: 0.439 accuracy: 84.375\n",
            "EPOCH: 14/100 idx: 200/391 Loss: 0.526 accuracy: 84.375\n",
            "EPOCH: 14/100 idx: 250/391 Loss: 0.585 accuracy: 79.688\n",
            "EPOCH: 14/100 idx: 300/391 Loss: 0.489 accuracy: 84.375\n",
            "EPOCH: 14/100 idx: 350/391 Loss: 0.551 accuracy: 82.031\n",
            "Test accuarcy: 80.08\n",
            "Test average loss: 0.01815123201906681\n",
            "Saving\n",
            "EPOCH: 15/100 idx: 50/391 Loss: 0.564 accuracy: 80.469\n",
            "EPOCH: 15/100 idx: 100/391 Loss: 0.612 accuracy: 80.469\n",
            "EPOCH: 15/100 idx: 150/391 Loss: 0.577 accuracy: 78.906\n",
            "EPOCH: 15/100 idx: 200/391 Loss: 0.695 accuracy: 78.125\n",
            "EPOCH: 15/100 idx: 250/391 Loss: 0.533 accuracy: 82.031\n",
            "EPOCH: 15/100 idx: 300/391 Loss: 0.628 accuracy: 78.906\n",
            "EPOCH: 15/100 idx: 350/391 Loss: 0.622 accuracy: 76.562\n",
            "Test accuarcy: 78.17\n",
            "Test average loss: 0.019952465170621873\n",
            "EPOCH: 16/100 idx: 50/391 Loss: 0.652 accuracy: 77.344\n",
            "EPOCH: 16/100 idx: 100/391 Loss: 0.457 accuracy: 82.031\n",
            "EPOCH: 16/100 idx: 150/391 Loss: 0.560 accuracy: 81.250\n",
            "EPOCH: 16/100 idx: 200/391 Loss: 0.624 accuracy: 80.469\n",
            "EPOCH: 16/100 idx: 250/391 Loss: 0.538 accuracy: 82.031\n",
            "EPOCH: 16/100 idx: 300/391 Loss: 0.604 accuracy: 78.125\n",
            "EPOCH: 16/100 idx: 350/391 Loss: 0.438 accuracy: 86.719\n",
            "Test accuarcy: 79.26\n",
            "Test average loss: 0.019222090555727482\n",
            "EPOCH: 17/100 idx: 50/391 Loss: 0.429 accuracy: 85.156\n",
            "EPOCH: 17/100 idx: 100/391 Loss: 0.451 accuracy: 84.375\n",
            "EPOCH: 17/100 idx: 150/391 Loss: 0.596 accuracy: 83.594\n",
            "EPOCH: 17/100 idx: 200/391 Loss: 0.632 accuracy: 81.250\n",
            "EPOCH: 17/100 idx: 250/391 Loss: 0.715 accuracy: 77.344\n",
            "EPOCH: 17/100 idx: 300/391 Loss: 0.489 accuracy: 84.375\n",
            "EPOCH: 17/100 idx: 350/391 Loss: 0.474 accuracy: 83.594\n",
            "Test accuarcy: 80.21\n",
            "Test average loss: 0.018209238962829114\n",
            "Saving\n",
            "EPOCH: 18/100 idx: 50/391 Loss: 0.445 accuracy: 83.594\n",
            "EPOCH: 18/100 idx: 100/391 Loss: 0.591 accuracy: 77.344\n",
            "EPOCH: 18/100 idx: 150/391 Loss: 0.524 accuracy: 82.031\n",
            "EPOCH: 18/100 idx: 200/391 Loss: 0.583 accuracy: 81.250\n",
            "EPOCH: 18/100 idx: 250/391 Loss: 0.630 accuracy: 80.469\n",
            "EPOCH: 18/100 idx: 300/391 Loss: 0.449 accuracy: 85.938\n",
            "EPOCH: 18/100 idx: 350/391 Loss: 0.471 accuracy: 83.594\n",
            "Test accuarcy: 81.26\n",
            "Test average loss: 0.01717532177567482\n",
            "Saving\n",
            "EPOCH: 19/100 idx: 50/391 Loss: 0.555 accuracy: 78.906\n",
            "EPOCH: 19/100 idx: 100/391 Loss: 0.408 accuracy: 82.812\n",
            "EPOCH: 19/100 idx: 150/391 Loss: 0.614 accuracy: 79.688\n",
            "EPOCH: 19/100 idx: 200/391 Loss: 0.535 accuracy: 82.812\n",
            "EPOCH: 19/100 idx: 250/391 Loss: 0.488 accuracy: 85.938\n",
            "EPOCH: 19/100 idx: 300/391 Loss: 0.423 accuracy: 84.375\n",
            "EPOCH: 19/100 idx: 350/391 Loss: 0.511 accuracy: 82.031\n",
            "Test accuarcy: 80.64\n",
            "Test average loss: 0.017925967314839364\n",
            "EPOCH: 20/100 idx: 50/391 Loss: 0.501 accuracy: 83.594\n",
            "EPOCH: 20/100 idx: 100/391 Loss: 0.526 accuracy: 81.250\n",
            "EPOCH: 20/100 idx: 150/391 Loss: 0.523 accuracy: 84.375\n",
            "EPOCH: 20/100 idx: 200/391 Loss: 0.412 accuracy: 84.375\n",
            "EPOCH: 20/100 idx: 250/391 Loss: 0.530 accuracy: 82.031\n",
            "EPOCH: 20/100 idx: 300/391 Loss: 0.680 accuracy: 78.906\n",
            "EPOCH: 20/100 idx: 350/391 Loss: 0.475 accuracy: 82.031\n",
            "Test accuarcy: 82.01\n",
            "Test average loss: 0.01631732797175646\n",
            "Saving\n",
            "EPOCH: 21/100 idx: 50/391 Loss: 0.414 accuracy: 86.719\n",
            "EPOCH: 21/100 idx: 100/391 Loss: 0.437 accuracy: 85.156\n",
            "EPOCH: 21/100 idx: 150/391 Loss: 0.458 accuracy: 82.812\n",
            "EPOCH: 21/100 idx: 200/391 Loss: 0.673 accuracy: 75.000\n",
            "EPOCH: 21/100 idx: 250/391 Loss: 0.528 accuracy: 82.031\n",
            "EPOCH: 21/100 idx: 300/391 Loss: 0.503 accuracy: 82.812\n",
            "EPOCH: 21/100 idx: 350/391 Loss: 0.505 accuracy: 78.906\n",
            "Test accuarcy: 82.24\n",
            "Test average loss: 0.01626284158527851\n",
            "Saving\n",
            "EPOCH: 22/100 idx: 50/391 Loss: 0.536 accuracy: 83.594\n",
            "EPOCH: 22/100 idx: 100/391 Loss: 0.493 accuracy: 83.594\n",
            "EPOCH: 22/100 idx: 150/391 Loss: 0.589 accuracy: 77.344\n",
            "EPOCH: 22/100 idx: 200/391 Loss: 0.529 accuracy: 84.375\n",
            "EPOCH: 22/100 idx: 250/391 Loss: 0.526 accuracy: 82.031\n",
            "EPOCH: 22/100 idx: 300/391 Loss: 0.501 accuracy: 83.594\n",
            "EPOCH: 22/100 idx: 350/391 Loss: 0.477 accuracy: 84.375\n",
            "Test accuarcy: 81.94\n",
            "Test average loss: 0.016447618965804576\n",
            "EPOCH: 23/100 idx: 50/391 Loss: 0.456 accuracy: 83.594\n",
            "EPOCH: 23/100 idx: 100/391 Loss: 0.407 accuracy: 84.375\n",
            "EPOCH: 23/100 idx: 150/391 Loss: 0.564 accuracy: 81.250\n",
            "EPOCH: 23/100 idx: 200/391 Loss: 0.480 accuracy: 80.469\n",
            "EPOCH: 23/100 idx: 250/391 Loss: 0.551 accuracy: 80.469\n",
            "EPOCH: 23/100 idx: 300/391 Loss: 0.628 accuracy: 76.562\n",
            "EPOCH: 23/100 idx: 350/391 Loss: 0.533 accuracy: 81.250\n",
            "Test accuarcy: 82.92\n",
            "Test average loss: 0.016271188884973527\n",
            "Saving\n",
            "EPOCH: 24/100 idx: 50/391 Loss: 0.548 accuracy: 80.469\n",
            "EPOCH: 24/100 idx: 100/391 Loss: 0.551 accuracy: 84.375\n",
            "EPOCH: 24/100 idx: 150/391 Loss: 0.506 accuracy: 83.594\n",
            "EPOCH: 24/100 idx: 200/391 Loss: 0.565 accuracy: 79.688\n",
            "EPOCH: 24/100 idx: 250/391 Loss: 0.449 accuracy: 82.812\n",
            "EPOCH: 24/100 idx: 300/391 Loss: 0.453 accuracy: 85.156\n",
            "EPOCH: 24/100 idx: 350/391 Loss: 0.322 accuracy: 90.625\n",
            "Test accuarcy: 81.94\n",
            "Test average loss: 0.01647367550432682\n",
            "EPOCH: 25/100 idx: 50/391 Loss: 0.351 accuracy: 87.500\n",
            "EPOCH: 25/100 idx: 100/391 Loss: 0.387 accuracy: 85.938\n",
            "EPOCH: 25/100 idx: 150/391 Loss: 0.458 accuracy: 86.719\n",
            "EPOCH: 25/100 idx: 200/391 Loss: 0.542 accuracy: 81.250\n",
            "EPOCH: 25/100 idx: 250/391 Loss: 0.596 accuracy: 85.156\n",
            "EPOCH: 25/100 idx: 300/391 Loss: 0.471 accuracy: 82.812\n",
            "EPOCH: 25/100 idx: 350/391 Loss: 0.571 accuracy: 77.344\n",
            "Test accuarcy: 82.92\n",
            "Test average loss: 0.01556520529538393\n",
            "EPOCH: 26/100 idx: 50/391 Loss: 0.376 accuracy: 88.281\n",
            "EPOCH: 26/100 idx: 100/391 Loss: 0.401 accuracy: 84.375\n",
            "EPOCH: 26/100 idx: 150/391 Loss: 0.580 accuracy: 80.469\n",
            "EPOCH: 26/100 idx: 200/391 Loss: 0.525 accuracy: 79.688\n",
            "EPOCH: 26/100 idx: 250/391 Loss: 0.584 accuracy: 81.250\n",
            "EPOCH: 26/100 idx: 300/391 Loss: 0.636 accuracy: 77.344\n",
            "EPOCH: 26/100 idx: 350/391 Loss: 0.466 accuracy: 84.375\n",
            "Test accuarcy: 82.6\n",
            "Test average loss: 0.016237779131531717\n",
            "EPOCH: 27/100 idx: 50/391 Loss: 0.318 accuracy: 88.281\n",
            "EPOCH: 27/100 idx: 100/391 Loss: 0.516 accuracy: 81.250\n",
            "EPOCH: 27/100 idx: 150/391 Loss: 0.463 accuracy: 85.938\n",
            "EPOCH: 27/100 idx: 200/391 Loss: 0.421 accuracy: 85.938\n",
            "EPOCH: 27/100 idx: 250/391 Loss: 0.582 accuracy: 81.250\n",
            "EPOCH: 27/100 idx: 300/391 Loss: 0.513 accuracy: 82.812\n",
            "EPOCH: 27/100 idx: 350/391 Loss: 0.443 accuracy: 87.500\n",
            "Test accuarcy: 83.55\n",
            "Test average loss: 0.014932033130526544\n",
            "Saving\n",
            "EPOCH: 28/100 idx: 50/391 Loss: 0.586 accuracy: 83.594\n",
            "EPOCH: 28/100 idx: 100/391 Loss: 0.469 accuracy: 82.812\n",
            "EPOCH: 28/100 idx: 150/391 Loss: 0.616 accuracy: 79.688\n",
            "EPOCH: 28/100 idx: 200/391 Loss: 0.478 accuracy: 86.719\n",
            "EPOCH: 28/100 idx: 250/391 Loss: 0.518 accuracy: 82.031\n",
            "EPOCH: 28/100 idx: 300/391 Loss: 0.329 accuracy: 89.062\n",
            "EPOCH: 28/100 idx: 350/391 Loss: 0.428 accuracy: 85.938\n",
            "Test accuarcy: 82.35\n",
            "Test average loss: 0.01628591147661209\n",
            "EPOCH: 29/100 idx: 50/391 Loss: 0.481 accuracy: 84.375\n",
            "EPOCH: 29/100 idx: 100/391 Loss: 0.474 accuracy: 86.719\n",
            "EPOCH: 29/100 idx: 150/391 Loss: 0.518 accuracy: 81.250\n",
            "EPOCH: 29/100 idx: 200/391 Loss: 0.450 accuracy: 85.156\n",
            "EPOCH: 29/100 idx: 250/391 Loss: 0.600 accuracy: 78.906\n",
            "EPOCH: 29/100 idx: 300/391 Loss: 0.467 accuracy: 82.812\n",
            "EPOCH: 29/100 idx: 350/391 Loss: 0.582 accuracy: 79.688\n",
            "Test accuarcy: 82.8\n",
            "Test average loss: 0.015750773591548205\n",
            "EPOCH: 30/100 idx: 50/391 Loss: 0.479 accuracy: 82.812\n",
            "EPOCH: 30/100 idx: 100/391 Loss: 0.426 accuracy: 85.938\n",
            "EPOCH: 30/100 idx: 150/391 Loss: 0.330 accuracy: 86.719\n",
            "EPOCH: 30/100 idx: 200/391 Loss: 0.402 accuracy: 86.719\n",
            "EPOCH: 30/100 idx: 250/391 Loss: 0.456 accuracy: 83.594\n",
            "EPOCH: 30/100 idx: 300/391 Loss: 0.383 accuracy: 87.500\n",
            "EPOCH: 30/100 idx: 350/391 Loss: 0.434 accuracy: 82.812\n",
            "Test accuarcy: 83.27\n",
            "Test average loss: 0.01556901289820671\n",
            "EPOCH: 31/100 idx: 50/391 Loss: 0.561 accuracy: 81.250\n",
            "EPOCH: 31/100 idx: 100/391 Loss: 0.464 accuracy: 85.156\n",
            "EPOCH: 31/100 idx: 150/391 Loss: 0.432 accuracy: 81.250\n",
            "EPOCH: 31/100 idx: 200/391 Loss: 0.480 accuracy: 84.375\n",
            "EPOCH: 31/100 idx: 250/391 Loss: 0.406 accuracy: 85.938\n",
            "EPOCH: 31/100 idx: 300/391 Loss: 0.478 accuracy: 85.156\n",
            "EPOCH: 31/100 idx: 350/391 Loss: 0.421 accuracy: 85.156\n",
            "Test accuarcy: 84.04\n",
            "Test average loss: 0.015053481248021125\n",
            "Saving\n",
            "EPOCH: 32/100 idx: 50/391 Loss: 0.442 accuracy: 83.594\n",
            "EPOCH: 32/100 idx: 100/391 Loss: 0.512 accuracy: 85.156\n",
            "EPOCH: 32/100 idx: 150/391 Loss: 0.439 accuracy: 85.156\n",
            "EPOCH: 32/100 idx: 200/391 Loss: 0.460 accuracy: 88.281\n",
            "EPOCH: 32/100 idx: 250/391 Loss: 0.407 accuracy: 81.250\n",
            "EPOCH: 32/100 idx: 300/391 Loss: 0.365 accuracy: 86.719\n",
            "EPOCH: 32/100 idx: 350/391 Loss: 0.496 accuracy: 83.594\n",
            "Test accuarcy: 83.15\n",
            "Test average loss: 0.015740818804502486\n",
            "EPOCH: 33/100 idx: 50/391 Loss: 0.461 accuracy: 82.812\n",
            "EPOCH: 33/100 idx: 100/391 Loss: 0.423 accuracy: 86.719\n",
            "EPOCH: 33/100 idx: 150/391 Loss: 0.547 accuracy: 81.250\n",
            "EPOCH: 33/100 idx: 200/391 Loss: 0.438 accuracy: 86.719\n",
            "EPOCH: 33/100 idx: 250/391 Loss: 0.403 accuracy: 84.375\n",
            "EPOCH: 33/100 idx: 300/391 Loss: 0.543 accuracy: 79.688\n",
            "EPOCH: 33/100 idx: 350/391 Loss: 0.301 accuracy: 91.406\n",
            "Test accuarcy: 84.12\n",
            "Test average loss: 0.014794451346993446\n",
            "Saving\n",
            "EPOCH: 34/100 idx: 50/391 Loss: 0.482 accuracy: 84.375\n",
            "EPOCH: 34/100 idx: 100/391 Loss: 0.351 accuracy: 87.500\n",
            "EPOCH: 34/100 idx: 150/391 Loss: 0.386 accuracy: 89.844\n",
            "EPOCH: 34/100 idx: 200/391 Loss: 0.614 accuracy: 79.688\n",
            "EPOCH: 34/100 idx: 250/391 Loss: 0.436 accuracy: 85.156\n",
            "EPOCH: 34/100 idx: 300/391 Loss: 0.541 accuracy: 82.031\n",
            "EPOCH: 34/100 idx: 350/391 Loss: 0.587 accuracy: 78.906\n",
            "Test accuarcy: 84.25\n",
            "Test average loss: 0.014890817764401435\n",
            "Saving\n",
            "EPOCH: 35/100 idx: 50/391 Loss: 0.351 accuracy: 89.062\n",
            "EPOCH: 35/100 idx: 100/391 Loss: 0.520 accuracy: 82.812\n",
            "EPOCH: 35/100 idx: 150/391 Loss: 0.387 accuracy: 85.156\n",
            "EPOCH: 35/100 idx: 200/391 Loss: 0.366 accuracy: 87.500\n",
            "EPOCH: 35/100 idx: 250/391 Loss: 0.331 accuracy: 87.500\n",
            "EPOCH: 35/100 idx: 300/391 Loss: 0.302 accuracy: 87.500\n",
            "EPOCH: 35/100 idx: 350/391 Loss: 0.339 accuracy: 88.281\n",
            "Test accuarcy: 83.45\n",
            "Test average loss: 0.015076543246209622\n",
            "EPOCH: 36/100 idx: 50/391 Loss: 0.386 accuracy: 88.281\n",
            "EPOCH: 36/100 idx: 100/391 Loss: 0.375 accuracy: 86.719\n",
            "EPOCH: 36/100 idx: 150/391 Loss: 0.459 accuracy: 84.375\n",
            "EPOCH: 36/100 idx: 200/391 Loss: 0.400 accuracy: 86.719\n",
            "EPOCH: 36/100 idx: 250/391 Loss: 0.273 accuracy: 91.406\n",
            "EPOCH: 36/100 idx: 300/391 Loss: 0.334 accuracy: 89.844\n",
            "EPOCH: 36/100 idx: 350/391 Loss: 0.351 accuracy: 89.062\n",
            "Test accuarcy: 82.84\n",
            "Test average loss: 0.015884347357600928\n",
            "EPOCH: 37/100 idx: 50/391 Loss: 0.395 accuracy: 83.594\n",
            "EPOCH: 37/100 idx: 100/391 Loss: 0.426 accuracy: 86.719\n",
            "EPOCH: 37/100 idx: 150/391 Loss: 0.484 accuracy: 82.812\n",
            "EPOCH: 37/100 idx: 200/391 Loss: 0.517 accuracy: 83.594\n",
            "EPOCH: 37/100 idx: 250/391 Loss: 0.373 accuracy: 87.500\n",
            "EPOCH: 37/100 idx: 300/391 Loss: 0.423 accuracy: 85.938\n",
            "EPOCH: 37/100 idx: 350/391 Loss: 0.488 accuracy: 81.250\n",
            "Test accuarcy: 84.08\n",
            "Test average loss: 0.014785580444335937\n",
            "EPOCH: 38/100 idx: 50/391 Loss: 0.334 accuracy: 89.844\n",
            "EPOCH: 38/100 idx: 100/391 Loss: 0.474 accuracy: 85.156\n",
            "EPOCH: 38/100 idx: 150/391 Loss: 0.512 accuracy: 85.156\n",
            "EPOCH: 38/100 idx: 200/391 Loss: 0.364 accuracy: 85.156\n",
            "EPOCH: 38/100 idx: 250/391 Loss: 0.382 accuracy: 87.500\n",
            "EPOCH: 38/100 idx: 300/391 Loss: 0.483 accuracy: 85.938\n",
            "EPOCH: 38/100 idx: 350/391 Loss: 0.362 accuracy: 84.375\n",
            "Test accuarcy: 83.45\n",
            "Test average loss: 0.015352319222688674\n",
            "EPOCH: 39/100 idx: 50/391 Loss: 0.497 accuracy: 80.469\n",
            "EPOCH: 39/100 idx: 100/391 Loss: 0.547 accuracy: 81.250\n",
            "EPOCH: 39/100 idx: 150/391 Loss: 0.350 accuracy: 87.500\n",
            "EPOCH: 39/100 idx: 200/391 Loss: 0.385 accuracy: 86.719\n",
            "EPOCH: 39/100 idx: 250/391 Loss: 0.421 accuracy: 86.719\n",
            "EPOCH: 39/100 idx: 300/391 Loss: 0.446 accuracy: 85.938\n",
            "EPOCH: 39/100 idx: 350/391 Loss: 0.420 accuracy: 83.594\n",
            "Test accuarcy: 84.3\n",
            "Test average loss: 0.014508407446742057\n",
            "Saving\n",
            "EPOCH: 40/100 idx: 50/391 Loss: 0.493 accuracy: 85.938\n",
            "EPOCH: 40/100 idx: 100/391 Loss: 0.274 accuracy: 90.625\n",
            "EPOCH: 40/100 idx: 150/391 Loss: 0.610 accuracy: 75.000\n",
            "EPOCH: 40/100 idx: 200/391 Loss: 0.337 accuracy: 90.625\n",
            "EPOCH: 40/100 idx: 250/391 Loss: 0.359 accuracy: 89.062\n",
            "EPOCH: 40/100 idx: 300/391 Loss: 0.555 accuracy: 81.250\n",
            "EPOCH: 40/100 idx: 350/391 Loss: 0.383 accuracy: 85.938\n",
            "Test accuarcy: 84.01\n",
            "Test average loss: 0.014911048065125942\n",
            "EPOCH: 41/100 idx: 50/391 Loss: 0.410 accuracy: 85.156\n",
            "EPOCH: 41/100 idx: 100/391 Loss: 0.298 accuracy: 91.406\n",
            "EPOCH: 41/100 idx: 150/391 Loss: 0.394 accuracy: 83.594\n",
            "EPOCH: 41/100 idx: 200/391 Loss: 0.415 accuracy: 89.062\n",
            "EPOCH: 41/100 idx: 250/391 Loss: 0.363 accuracy: 85.938\n",
            "EPOCH: 41/100 idx: 300/391 Loss: 0.427 accuracy: 84.375\n",
            "EPOCH: 41/100 idx: 350/391 Loss: 0.349 accuracy: 90.625\n",
            "Test accuarcy: 84.03\n",
            "Test average loss: 0.015185721662640572\n",
            "EPOCH: 42/100 idx: 50/391 Loss: 0.454 accuracy: 84.375\n",
            "EPOCH: 42/100 idx: 100/391 Loss: 0.413 accuracy: 85.938\n",
            "EPOCH: 42/100 idx: 150/391 Loss: 0.438 accuracy: 89.062\n",
            "EPOCH: 42/100 idx: 200/391 Loss: 0.283 accuracy: 87.500\n",
            "EPOCH: 42/100 idx: 250/391 Loss: 0.502 accuracy: 83.594\n",
            "EPOCH: 42/100 idx: 300/391 Loss: 0.300 accuracy: 89.844\n",
            "EPOCH: 42/100 idx: 350/391 Loss: 0.587 accuracy: 81.250\n",
            "Test accuarcy: 85.06\n",
            "Test average loss: 0.013879129181057215\n",
            "Saving\n",
            "EPOCH: 43/100 idx: 50/391 Loss: 0.402 accuracy: 85.938\n",
            "EPOCH: 43/100 idx: 100/391 Loss: 0.641 accuracy: 81.250\n",
            "EPOCH: 43/100 idx: 150/391 Loss: 0.371 accuracy: 90.625\n",
            "EPOCH: 43/100 idx: 200/391 Loss: 0.501 accuracy: 82.031\n",
            "EPOCH: 43/100 idx: 250/391 Loss: 0.439 accuracy: 83.594\n",
            "EPOCH: 43/100 idx: 300/391 Loss: 0.340 accuracy: 88.281\n",
            "EPOCH: 43/100 idx: 350/391 Loss: 0.292 accuracy: 87.500\n",
            "Test accuarcy: 83.99\n",
            "Test average loss: 0.014938936863094568\n",
            "EPOCH: 44/100 idx: 50/391 Loss: 0.288 accuracy: 91.406\n",
            "EPOCH: 44/100 idx: 100/391 Loss: 0.376 accuracy: 83.594\n",
            "EPOCH: 44/100 idx: 150/391 Loss: 0.335 accuracy: 90.625\n",
            "EPOCH: 44/100 idx: 200/391 Loss: 0.586 accuracy: 81.250\n",
            "EPOCH: 44/100 idx: 250/391 Loss: 0.328 accuracy: 89.062\n",
            "EPOCH: 44/100 idx: 300/391 Loss: 0.422 accuracy: 85.938\n",
            "EPOCH: 44/100 idx: 350/391 Loss: 0.331 accuracy: 88.281\n",
            "Test accuarcy: 84.89\n",
            "Test average loss: 0.014219961141049862\n",
            "EPOCH: 45/100 idx: 50/391 Loss: 0.323 accuracy: 87.500\n",
            "EPOCH: 45/100 idx: 100/391 Loss: 0.339 accuracy: 88.281\n",
            "EPOCH: 45/100 idx: 150/391 Loss: 0.412 accuracy: 87.500\n",
            "EPOCH: 45/100 idx: 200/391 Loss: 0.421 accuracy: 87.500\n",
            "EPOCH: 45/100 idx: 250/391 Loss: 0.403 accuracy: 84.375\n",
            "EPOCH: 45/100 idx: 300/391 Loss: 0.219 accuracy: 92.188\n",
            "EPOCH: 45/100 idx: 350/391 Loss: 0.502 accuracy: 80.469\n",
            "Test accuarcy: 84.7\n",
            "Test average loss: 0.014174226509034634\n",
            "EPOCH: 46/100 idx: 50/391 Loss: 0.352 accuracy: 88.281\n",
            "EPOCH: 46/100 idx: 100/391 Loss: 0.390 accuracy: 87.500\n",
            "EPOCH: 46/100 idx: 150/391 Loss: 0.437 accuracy: 85.938\n",
            "EPOCH: 46/100 idx: 200/391 Loss: 0.449 accuracy: 85.938\n",
            "EPOCH: 46/100 idx: 250/391 Loss: 0.360 accuracy: 88.281\n",
            "EPOCH: 46/100 idx: 300/391 Loss: 0.313 accuracy: 89.844\n",
            "EPOCH: 46/100 idx: 350/391 Loss: 0.410 accuracy: 85.938\n",
            "Test accuarcy: 84.56\n",
            "Test average loss: 0.014351016300171614\n",
            "EPOCH: 47/100 idx: 50/391 Loss: 0.434 accuracy: 85.156\n",
            "EPOCH: 47/100 idx: 100/391 Loss: 0.621 accuracy: 77.344\n",
            "EPOCH: 47/100 idx: 150/391 Loss: 0.333 accuracy: 89.844\n",
            "EPOCH: 47/100 idx: 200/391 Loss: 0.482 accuracy: 83.594\n",
            "EPOCH: 47/100 idx: 250/391 Loss: 0.297 accuracy: 90.625\n",
            "EPOCH: 47/100 idx: 300/391 Loss: 0.349 accuracy: 89.062\n",
            "EPOCH: 47/100 idx: 350/391 Loss: 0.380 accuracy: 91.406\n",
            "Test accuarcy: 84.53\n",
            "Test average loss: 0.01423554632589221\n",
            "EPOCH: 48/100 idx: 50/391 Loss: 0.405 accuracy: 84.375\n",
            "EPOCH: 48/100 idx: 100/391 Loss: 0.545 accuracy: 83.594\n",
            "EPOCH: 48/100 idx: 150/391 Loss: 0.371 accuracy: 88.281\n",
            "EPOCH: 48/100 idx: 200/391 Loss: 0.469 accuracy: 85.156\n",
            "EPOCH: 48/100 idx: 250/391 Loss: 0.328 accuracy: 88.281\n",
            "EPOCH: 48/100 idx: 300/391 Loss: 0.269 accuracy: 89.844\n",
            "EPOCH: 48/100 idx: 350/391 Loss: 0.405 accuracy: 85.156\n",
            "Test accuarcy: 84.31\n",
            "Test average loss: 0.014282991159707308\n",
            "EPOCH: 49/100 idx: 50/391 Loss: 0.319 accuracy: 90.625\n",
            "EPOCH: 49/100 idx: 100/391 Loss: 0.417 accuracy: 86.719\n",
            "EPOCH: 49/100 idx: 150/391 Loss: 0.399 accuracy: 85.156\n",
            "EPOCH: 49/100 idx: 200/391 Loss: 0.315 accuracy: 89.844\n",
            "EPOCH: 49/100 idx: 250/391 Loss: 0.389 accuracy: 82.812\n",
            "EPOCH: 49/100 idx: 300/391 Loss: 0.479 accuracy: 85.156\n",
            "EPOCH: 49/100 idx: 350/391 Loss: 0.546 accuracy: 83.594\n",
            "Test accuarcy: 84.24\n",
            "Test average loss: 0.014368489944189787\n",
            "EPOCH: 50/100 idx: 50/391 Loss: 0.491 accuracy: 84.375\n",
            "EPOCH: 50/100 idx: 100/391 Loss: 0.364 accuracy: 85.156\n",
            "EPOCH: 50/100 idx: 150/391 Loss: 0.277 accuracy: 88.281\n",
            "EPOCH: 50/100 idx: 200/391 Loss: 0.333 accuracy: 88.281\n",
            "EPOCH: 50/100 idx: 250/391 Loss: 0.416 accuracy: 88.281\n",
            "EPOCH: 50/100 idx: 300/391 Loss: 0.369 accuracy: 89.844\n",
            "EPOCH: 50/100 idx: 350/391 Loss: 0.367 accuracy: 86.719\n",
            "Test accuarcy: 83.81\n",
            "Test average loss: 0.015103956203907727\n",
            "EPOCH: 51/100 idx: 50/391 Loss: 0.387 accuracy: 89.844\n",
            "EPOCH: 51/100 idx: 100/391 Loss: 0.404 accuracy: 89.062\n",
            "EPOCH: 51/100 idx: 150/391 Loss: 0.275 accuracy: 92.188\n",
            "EPOCH: 51/100 idx: 200/391 Loss: 0.395 accuracy: 90.625\n",
            "EPOCH: 51/100 idx: 250/391 Loss: 0.435 accuracy: 82.812\n",
            "EPOCH: 51/100 idx: 300/391 Loss: 0.419 accuracy: 85.938\n",
            "EPOCH: 51/100 idx: 350/391 Loss: 0.331 accuracy: 87.500\n",
            "Test accuarcy: 85.07\n",
            "Test average loss: 0.014029935801029205\n",
            "Saving\n",
            "EPOCH: 52/100 idx: 50/391 Loss: 0.181 accuracy: 94.531\n",
            "EPOCH: 52/100 idx: 100/391 Loss: 0.345 accuracy: 87.500\n",
            "EPOCH: 52/100 idx: 150/391 Loss: 0.342 accuracy: 86.719\n",
            "EPOCH: 52/100 idx: 200/391 Loss: 0.283 accuracy: 91.406\n",
            "EPOCH: 52/100 idx: 250/391 Loss: 0.271 accuracy: 89.844\n",
            "EPOCH: 52/100 idx: 300/391 Loss: 0.359 accuracy: 85.938\n",
            "EPOCH: 52/100 idx: 350/391 Loss: 0.364 accuracy: 87.500\n",
            "Test accuarcy: 84.21\n",
            "Test average loss: 0.014548727125674486\n",
            "EPOCH: 53/100 idx: 50/391 Loss: 0.347 accuracy: 89.844\n",
            "EPOCH: 53/100 idx: 100/391 Loss: 0.320 accuracy: 89.062\n",
            "EPOCH: 53/100 idx: 150/391 Loss: 0.453 accuracy: 82.031\n",
            "EPOCH: 53/100 idx: 200/391 Loss: 0.422 accuracy: 85.156\n",
            "EPOCH: 53/100 idx: 250/391 Loss: 0.401 accuracy: 86.719\n",
            "EPOCH: 53/100 idx: 300/391 Loss: 0.393 accuracy: 86.719\n",
            "EPOCH: 53/100 idx: 350/391 Loss: 0.326 accuracy: 86.719\n",
            "Test accuarcy: 85.33\n",
            "Test average loss: 0.013617987073212863\n",
            "Saving\n",
            "EPOCH: 54/100 idx: 50/391 Loss: 0.304 accuracy: 87.500\n",
            "EPOCH: 54/100 idx: 100/391 Loss: 0.359 accuracy: 89.062\n",
            "EPOCH: 54/100 idx: 150/391 Loss: 0.344 accuracy: 87.500\n",
            "EPOCH: 54/100 idx: 200/391 Loss: 0.501 accuracy: 85.156\n",
            "EPOCH: 54/100 idx: 250/391 Loss: 0.221 accuracy: 92.969\n",
            "EPOCH: 54/100 idx: 300/391 Loss: 0.285 accuracy: 91.406\n",
            "EPOCH: 54/100 idx: 350/391 Loss: 0.364 accuracy: 87.500\n",
            "Test accuarcy: 84.52\n",
            "Test average loss: 0.01478231494948268\n",
            "EPOCH: 55/100 idx: 50/391 Loss: 0.327 accuracy: 90.625\n",
            "EPOCH: 55/100 idx: 100/391 Loss: 0.387 accuracy: 89.062\n",
            "EPOCH: 55/100 idx: 150/391 Loss: 0.422 accuracy: 85.156\n",
            "EPOCH: 55/100 idx: 200/391 Loss: 0.448 accuracy: 87.500\n",
            "EPOCH: 55/100 idx: 250/391 Loss: 0.298 accuracy: 88.281\n",
            "EPOCH: 55/100 idx: 300/391 Loss: 0.259 accuracy: 89.062\n",
            "EPOCH: 55/100 idx: 350/391 Loss: 0.265 accuracy: 95.312\n",
            "Test accuarcy: 84.79\n",
            "Test average loss: 0.014032779105752707\n",
            "EPOCH: 56/100 idx: 50/391 Loss: 0.371 accuracy: 85.156\n",
            "EPOCH: 56/100 idx: 100/391 Loss: 0.297 accuracy: 89.844\n",
            "EPOCH: 56/100 idx: 150/391 Loss: 0.492 accuracy: 83.594\n",
            "EPOCH: 56/100 idx: 200/391 Loss: 0.398 accuracy: 88.281\n",
            "EPOCH: 56/100 idx: 250/391 Loss: 0.423 accuracy: 85.938\n",
            "EPOCH: 56/100 idx: 300/391 Loss: 0.373 accuracy: 88.281\n",
            "EPOCH: 56/100 idx: 350/391 Loss: 0.359 accuracy: 85.156\n",
            "Test accuarcy: 84.83\n",
            "Test average loss: 0.01389046570956707\n",
            "EPOCH: 57/100 idx: 50/391 Loss: 0.325 accuracy: 89.844\n",
            "EPOCH: 57/100 idx: 100/391 Loss: 0.512 accuracy: 84.375\n",
            "EPOCH: 57/100 idx: 150/391 Loss: 0.244 accuracy: 93.750\n",
            "EPOCH: 57/100 idx: 200/391 Loss: 0.370 accuracy: 88.281\n",
            "EPOCH: 57/100 idx: 250/391 Loss: 0.336 accuracy: 86.719\n",
            "EPOCH: 57/100 idx: 300/391 Loss: 0.301 accuracy: 90.625\n",
            "EPOCH: 57/100 idx: 350/391 Loss: 0.467 accuracy: 85.156\n",
            "Test accuarcy: 85.04\n",
            "Test average loss: 0.01381562535315752\n",
            "EPOCH: 58/100 idx: 50/391 Loss: 0.304 accuracy: 89.844\n",
            "EPOCH: 58/100 idx: 100/391 Loss: 0.422 accuracy: 84.375\n",
            "EPOCH: 58/100 idx: 150/391 Loss: 0.468 accuracy: 85.938\n",
            "EPOCH: 58/100 idx: 200/391 Loss: 0.345 accuracy: 85.156\n",
            "EPOCH: 58/100 idx: 250/391 Loss: 0.215 accuracy: 91.406\n",
            "EPOCH: 58/100 idx: 300/391 Loss: 0.380 accuracy: 88.281\n",
            "EPOCH: 58/100 idx: 350/391 Loss: 0.371 accuracy: 87.500\n",
            "Test accuarcy: 86.09\n",
            "Test average loss: 0.013162515919655561\n",
            "Saving\n",
            "EPOCH: 59/100 idx: 50/391 Loss: 0.395 accuracy: 86.719\n",
            "EPOCH: 59/100 idx: 100/391 Loss: 0.400 accuracy: 85.938\n",
            "EPOCH: 59/100 idx: 150/391 Loss: 0.357 accuracy: 86.719\n",
            "EPOCH: 59/100 idx: 200/391 Loss: 0.183 accuracy: 94.531\n",
            "EPOCH: 59/100 idx: 250/391 Loss: 0.263 accuracy: 89.062\n",
            "EPOCH: 59/100 idx: 300/391 Loss: 0.439 accuracy: 82.812\n",
            "EPOCH: 59/100 idx: 350/391 Loss: 0.456 accuracy: 87.500\n",
            "Test accuarcy: 85.29\n",
            "Test average loss: 0.013702548407763243\n",
            "EPOCH: 60/100 idx: 50/391 Loss: 0.218 accuracy: 94.531\n",
            "EPOCH: 60/100 idx: 100/391 Loss: 0.431 accuracy: 82.031\n",
            "EPOCH: 60/100 idx: 150/391 Loss: 0.307 accuracy: 88.281\n",
            "EPOCH: 60/100 idx: 200/391 Loss: 0.296 accuracy: 88.281\n",
            "EPOCH: 60/100 idx: 250/391 Loss: 0.295 accuracy: 90.625\n",
            "EPOCH: 60/100 idx: 300/391 Loss: 0.244 accuracy: 89.062\n",
            "EPOCH: 60/100 idx: 350/391 Loss: 0.354 accuracy: 85.938\n",
            "Test accuarcy: 85.81\n",
            "Test average loss: 0.01299101708009839\n",
            "EPOCH: 61/100 idx: 50/391 Loss: 0.276 accuracy: 90.625\n",
            "EPOCH: 61/100 idx: 100/391 Loss: 0.335 accuracy: 91.406\n",
            "EPOCH: 61/100 idx: 150/391 Loss: 0.280 accuracy: 89.844\n",
            "EPOCH: 61/100 idx: 200/391 Loss: 0.367 accuracy: 89.062\n",
            "EPOCH: 61/100 idx: 250/391 Loss: 0.291 accuracy: 90.625\n",
            "EPOCH: 61/100 idx: 300/391 Loss: 0.362 accuracy: 87.500\n",
            "EPOCH: 61/100 idx: 350/391 Loss: 0.353 accuracy: 87.500\n",
            "Test accuarcy: 85.83\n",
            "Test average loss: 0.013066844802349806\n",
            "EPOCH: 62/100 idx: 50/391 Loss: 0.215 accuracy: 93.750\n",
            "EPOCH: 62/100 idx: 100/391 Loss: 0.407 accuracy: 86.719\n",
            "EPOCH: 62/100 idx: 150/391 Loss: 0.336 accuracy: 89.062\n",
            "EPOCH: 62/100 idx: 200/391 Loss: 0.392 accuracy: 85.938\n",
            "EPOCH: 62/100 idx: 250/391 Loss: 0.401 accuracy: 81.250\n",
            "EPOCH: 62/100 idx: 300/391 Loss: 0.379 accuracy: 87.500\n",
            "EPOCH: 62/100 idx: 350/391 Loss: 0.299 accuracy: 90.625\n",
            "Test accuarcy: 85.77\n",
            "Test average loss: 0.01348965814486146\n",
            "EPOCH: 63/100 idx: 50/391 Loss: 0.344 accuracy: 87.500\n",
            "EPOCH: 63/100 idx: 100/391 Loss: 0.310 accuracy: 88.281\n",
            "EPOCH: 63/100 idx: 150/391 Loss: 0.234 accuracy: 92.188\n",
            "EPOCH: 63/100 idx: 200/391 Loss: 0.309 accuracy: 89.844\n",
            "EPOCH: 63/100 idx: 250/391 Loss: 0.346 accuracy: 88.281\n",
            "EPOCH: 63/100 idx: 300/391 Loss: 0.318 accuracy: 89.844\n",
            "EPOCH: 63/100 idx: 350/391 Loss: 0.344 accuracy: 86.719\n",
            "Test accuarcy: 84.82\n",
            "Test average loss: 0.014233360026776791\n",
            "EPOCH: 64/100 idx: 50/391 Loss: 0.337 accuracy: 85.938\n",
            "EPOCH: 64/100 idx: 100/391 Loss: 0.438 accuracy: 88.281\n",
            "EPOCH: 64/100 idx: 150/391 Loss: 0.241 accuracy: 91.406\n",
            "EPOCH: 64/100 idx: 200/391 Loss: 0.222 accuracy: 92.969\n",
            "EPOCH: 64/100 idx: 250/391 Loss: 0.377 accuracy: 87.500\n",
            "EPOCH: 64/100 idx: 300/391 Loss: 0.430 accuracy: 84.375\n",
            "EPOCH: 64/100 idx: 350/391 Loss: 0.330 accuracy: 89.062\n",
            "Test accuarcy: 85.45\n",
            "Test average loss: 0.01332240265160799\n",
            "EPOCH: 65/100 idx: 50/391 Loss: 0.343 accuracy: 89.062\n",
            "EPOCH: 65/100 idx: 100/391 Loss: 0.324 accuracy: 89.844\n",
            "EPOCH: 65/100 idx: 150/391 Loss: 0.317 accuracy: 88.281\n",
            "EPOCH: 65/100 idx: 200/391 Loss: 0.322 accuracy: 85.938\n",
            "EPOCH: 65/100 idx: 250/391 Loss: 0.354 accuracy: 89.844\n",
            "EPOCH: 65/100 idx: 300/391 Loss: 0.358 accuracy: 89.062\n",
            "EPOCH: 65/100 idx: 350/391 Loss: 0.397 accuracy: 83.594\n",
            "Test accuarcy: 85.47\n",
            "Test average loss: 0.01357106107994914\n",
            "EPOCH: 66/100 idx: 50/391 Loss: 0.370 accuracy: 89.062\n",
            "EPOCH: 66/100 idx: 100/391 Loss: 0.356 accuracy: 85.938\n",
            "EPOCH: 66/100 idx: 150/391 Loss: 0.396 accuracy: 85.156\n",
            "EPOCH: 66/100 idx: 200/391 Loss: 0.266 accuracy: 89.062\n",
            "EPOCH: 66/100 idx: 250/391 Loss: 0.304 accuracy: 91.406\n",
            "EPOCH: 66/100 idx: 300/391 Loss: 0.301 accuracy: 91.406\n",
            "EPOCH: 66/100 idx: 350/391 Loss: 0.383 accuracy: 85.938\n",
            "Test accuarcy: 86.22\n",
            "Test average loss: 0.012891965013742446\n",
            "Saving\n",
            "EPOCH: 67/100 idx: 50/391 Loss: 0.178 accuracy: 95.312\n",
            "EPOCH: 67/100 idx: 100/391 Loss: 0.342 accuracy: 89.844\n",
            "EPOCH: 67/100 idx: 150/391 Loss: 0.417 accuracy: 85.156\n",
            "EPOCH: 67/100 idx: 200/391 Loss: 0.265 accuracy: 92.188\n",
            "EPOCH: 67/100 idx: 250/391 Loss: 0.469 accuracy: 85.156\n",
            "EPOCH: 67/100 idx: 300/391 Loss: 0.376 accuracy: 85.938\n",
            "EPOCH: 67/100 idx: 350/391 Loss: 0.340 accuracy: 89.844\n",
            "Test accuarcy: 85.09\n",
            "Test average loss: 0.013831994991004466\n",
            "EPOCH: 68/100 idx: 50/391 Loss: 0.353 accuracy: 89.062\n",
            "EPOCH: 68/100 idx: 100/391 Loss: 0.243 accuracy: 92.188\n",
            "EPOCH: 68/100 idx: 150/391 Loss: 0.371 accuracy: 88.281\n",
            "EPOCH: 68/100 idx: 200/391 Loss: 0.285 accuracy: 88.281\n",
            "EPOCH: 68/100 idx: 250/391 Loss: 0.284 accuracy: 89.844\n",
            "EPOCH: 68/100 idx: 300/391 Loss: 0.257 accuracy: 89.844\n",
            "EPOCH: 68/100 idx: 350/391 Loss: 0.236 accuracy: 91.406\n",
            "Test accuarcy: 85.3\n",
            "Test average loss: 0.013870922923833132\n",
            "EPOCH: 69/100 idx: 50/391 Loss: 0.358 accuracy: 87.500\n",
            "EPOCH: 69/100 idx: 100/391 Loss: 0.284 accuracy: 92.969\n",
            "EPOCH: 69/100 idx: 150/391 Loss: 0.211 accuracy: 90.625\n",
            "EPOCH: 69/100 idx: 200/391 Loss: 0.261 accuracy: 92.969\n",
            "EPOCH: 69/100 idx: 250/391 Loss: 0.245 accuracy: 90.625\n",
            "EPOCH: 69/100 idx: 300/391 Loss: 0.248 accuracy: 91.406\n",
            "EPOCH: 69/100 idx: 350/391 Loss: 0.380 accuracy: 86.719\n",
            "Test accuarcy: 85.15\n",
            "Test average loss: 0.013661311137676239\n",
            "EPOCH: 70/100 idx: 50/391 Loss: 0.379 accuracy: 89.844\n",
            "EPOCH: 70/100 idx: 100/391 Loss: 0.274 accuracy: 90.625\n",
            "EPOCH: 70/100 idx: 150/391 Loss: 0.306 accuracy: 92.969\n",
            "EPOCH: 70/100 idx: 200/391 Loss: 0.430 accuracy: 89.062\n",
            "EPOCH: 70/100 idx: 250/391 Loss: 0.483 accuracy: 84.375\n",
            "EPOCH: 70/100 idx: 300/391 Loss: 0.395 accuracy: 89.844\n",
            "EPOCH: 70/100 idx: 350/391 Loss: 0.287 accuracy: 91.406\n",
            "Test accuarcy: 86.2\n",
            "Test average loss: 0.012904540163278579\n",
            "EPOCH: 71/100 idx: 50/391 Loss: 0.339 accuracy: 86.719\n",
            "EPOCH: 71/100 idx: 100/391 Loss: 0.321 accuracy: 89.844\n",
            "EPOCH: 71/100 idx: 150/391 Loss: 0.284 accuracy: 88.281\n",
            "EPOCH: 71/100 idx: 200/391 Loss: 0.253 accuracy: 89.844\n",
            "EPOCH: 71/100 idx: 250/391 Loss: 0.266 accuracy: 91.406\n",
            "EPOCH: 71/100 idx: 300/391 Loss: 0.334 accuracy: 89.844\n",
            "EPOCH: 71/100 idx: 350/391 Loss: 0.449 accuracy: 83.594\n",
            "Test accuarcy: 85.88\n",
            "Test average loss: 0.013024266200512648\n",
            "EPOCH: 72/100 idx: 50/391 Loss: 0.243 accuracy: 86.719\n",
            "EPOCH: 72/100 idx: 100/391 Loss: 0.402 accuracy: 86.719\n",
            "EPOCH: 72/100 idx: 150/391 Loss: 0.346 accuracy: 85.938\n",
            "EPOCH: 72/100 idx: 200/391 Loss: 0.456 accuracy: 85.938\n",
            "EPOCH: 72/100 idx: 250/391 Loss: 0.383 accuracy: 87.500\n",
            "EPOCH: 72/100 idx: 300/391 Loss: 0.327 accuracy: 87.500\n",
            "EPOCH: 72/100 idx: 350/391 Loss: 0.276 accuracy: 92.188\n",
            "Test accuarcy: 85.94\n",
            "Test average loss: 0.01338580402955413\n",
            "EPOCH: 73/100 idx: 50/391 Loss: 0.256 accuracy: 90.625\n",
            "EPOCH: 73/100 idx: 100/391 Loss: 0.438 accuracy: 82.812\n",
            "EPOCH: 73/100 idx: 150/391 Loss: 0.316 accuracy: 91.406\n",
            "EPOCH: 73/100 idx: 200/391 Loss: 0.339 accuracy: 87.500\n",
            "EPOCH: 73/100 idx: 250/391 Loss: 0.384 accuracy: 86.719\n",
            "EPOCH: 73/100 idx: 300/391 Loss: 0.364 accuracy: 85.938\n",
            "EPOCH: 73/100 idx: 350/391 Loss: 0.358 accuracy: 87.500\n",
            "Test accuarcy: 85.35\n",
            "Test average loss: 0.013579162472486497\n",
            "EPOCH: 74/100 idx: 50/391 Loss: 0.397 accuracy: 85.938\n",
            "EPOCH: 74/100 idx: 100/391 Loss: 0.245 accuracy: 90.625\n",
            "EPOCH: 74/100 idx: 150/391 Loss: 0.459 accuracy: 84.375\n",
            "EPOCH: 74/100 idx: 200/391 Loss: 0.174 accuracy: 95.312\n",
            "EPOCH: 74/100 idx: 250/391 Loss: 0.367 accuracy: 87.500\n",
            "EPOCH: 74/100 idx: 300/391 Loss: 0.338 accuracy: 88.281\n",
            "EPOCH: 74/100 idx: 350/391 Loss: 0.310 accuracy: 89.844\n",
            "Test accuarcy: 85.28\n",
            "Test average loss: 0.013720938044786453\n",
            "EPOCH: 75/100 idx: 50/391 Loss: 0.279 accuracy: 88.281\n",
            "EPOCH: 75/100 idx: 100/391 Loss: 0.301 accuracy: 90.625\n",
            "EPOCH: 75/100 idx: 150/391 Loss: 0.327 accuracy: 88.281\n",
            "EPOCH: 75/100 idx: 200/391 Loss: 0.303 accuracy: 91.406\n",
            "EPOCH: 75/100 idx: 250/391 Loss: 0.406 accuracy: 85.156\n",
            "EPOCH: 75/100 idx: 300/391 Loss: 0.335 accuracy: 89.062\n",
            "EPOCH: 75/100 idx: 350/391 Loss: 0.330 accuracy: 90.625\n",
            "Test accuarcy: 85.6\n",
            "Test average loss: 0.013227088645100594\n",
            "EPOCH: 76/100 idx: 50/391 Loss: 0.414 accuracy: 82.812\n",
            "EPOCH: 76/100 idx: 100/391 Loss: 0.237 accuracy: 91.406\n",
            "EPOCH: 76/100 idx: 150/391 Loss: 0.345 accuracy: 89.844\n",
            "EPOCH: 76/100 idx: 200/391 Loss: 0.401 accuracy: 85.938\n",
            "EPOCH: 76/100 idx: 250/391 Loss: 0.293 accuracy: 88.281\n",
            "EPOCH: 76/100 idx: 300/391 Loss: 0.355 accuracy: 86.719\n",
            "EPOCH: 76/100 idx: 350/391 Loss: 0.268 accuracy: 90.625\n",
            "Test accuarcy: 86.3\n",
            "Test average loss: 0.01311896882429719\n",
            "Saving\n",
            "EPOCH: 77/100 idx: 50/391 Loss: 0.254 accuracy: 92.188\n",
            "EPOCH: 77/100 idx: 100/391 Loss: 0.276 accuracy: 91.406\n",
            "EPOCH: 77/100 idx: 150/391 Loss: 0.305 accuracy: 87.500\n",
            "EPOCH: 77/100 idx: 200/391 Loss: 0.246 accuracy: 91.406\n",
            "EPOCH: 77/100 idx: 250/391 Loss: 0.307 accuracy: 89.844\n",
            "EPOCH: 77/100 idx: 300/391 Loss: 0.245 accuracy: 89.844\n",
            "EPOCH: 77/100 idx: 350/391 Loss: 0.452 accuracy: 85.156\n",
            "Test accuarcy: 85.31\n",
            "Test average loss: 0.013510756173729897\n",
            "EPOCH: 78/100 idx: 50/391 Loss: 0.272 accuracy: 91.406\n",
            "EPOCH: 78/100 idx: 100/391 Loss: 0.222 accuracy: 90.625\n",
            "EPOCH: 78/100 idx: 150/391 Loss: 0.291 accuracy: 90.625\n",
            "EPOCH: 78/100 idx: 200/391 Loss: 0.292 accuracy: 88.281\n",
            "EPOCH: 78/100 idx: 250/391 Loss: 0.349 accuracy: 85.156\n",
            "EPOCH: 78/100 idx: 300/391 Loss: 0.201 accuracy: 93.750\n",
            "EPOCH: 78/100 idx: 350/391 Loss: 0.315 accuracy: 89.062\n",
            "Test accuarcy: 84.99\n",
            "Test average loss: 0.014338676756247878\n",
            "EPOCH: 79/100 idx: 50/391 Loss: 0.273 accuracy: 90.625\n",
            "EPOCH: 79/100 idx: 100/391 Loss: 0.348 accuracy: 87.500\n",
            "EPOCH: 79/100 idx: 150/391 Loss: 0.452 accuracy: 82.812\n",
            "EPOCH: 79/100 idx: 200/391 Loss: 0.363 accuracy: 85.156\n",
            "EPOCH: 79/100 idx: 250/391 Loss: 0.213 accuracy: 91.406\n",
            "EPOCH: 79/100 idx: 300/391 Loss: 0.236 accuracy: 92.188\n",
            "EPOCH: 79/100 idx: 350/391 Loss: 0.322 accuracy: 91.406\n",
            "Test accuarcy: 85.95\n",
            "Test average loss: 0.013219128269702196\n",
            "EPOCH: 80/100 idx: 50/391 Loss: 0.403 accuracy: 89.062\n",
            "EPOCH: 80/100 idx: 100/391 Loss: 0.386 accuracy: 84.375\n",
            "EPOCH: 80/100 idx: 150/391 Loss: 0.185 accuracy: 93.750\n",
            "EPOCH: 80/100 idx: 200/391 Loss: 0.221 accuracy: 92.188\n",
            "EPOCH: 80/100 idx: 250/391 Loss: 0.254 accuracy: 92.188\n",
            "EPOCH: 80/100 idx: 300/391 Loss: 0.257 accuracy: 92.188\n",
            "EPOCH: 80/100 idx: 350/391 Loss: 0.206 accuracy: 90.625\n",
            "Test accuarcy: 85.74\n",
            "Test average loss: 0.013340187305957079\n",
            "EPOCH: 81/100 idx: 50/391 Loss: 0.247 accuracy: 92.188\n",
            "EPOCH: 81/100 idx: 100/391 Loss: 0.244 accuracy: 90.625\n",
            "EPOCH: 81/100 idx: 150/391 Loss: 0.207 accuracy: 92.969\n",
            "EPOCH: 81/100 idx: 200/391 Loss: 0.295 accuracy: 90.625\n",
            "EPOCH: 81/100 idx: 250/391 Loss: 0.256 accuracy: 91.406\n",
            "EPOCH: 81/100 idx: 300/391 Loss: 0.372 accuracy: 89.062\n",
            "EPOCH: 81/100 idx: 350/391 Loss: 0.266 accuracy: 90.625\n",
            "Test accuarcy: 85.87\n",
            "Test average loss: 0.013114706566184759\n",
            "EPOCH: 82/100 idx: 50/391 Loss: 0.293 accuracy: 89.062\n",
            "EPOCH: 82/100 idx: 100/391 Loss: 0.276 accuracy: 92.969\n",
            "EPOCH: 82/100 idx: 150/391 Loss: 0.344 accuracy: 88.281\n",
            "EPOCH: 82/100 idx: 200/391 Loss: 0.461 accuracy: 82.812\n",
            "EPOCH: 82/100 idx: 250/391 Loss: 0.253 accuracy: 92.188\n",
            "EPOCH: 82/100 idx: 300/391 Loss: 0.351 accuracy: 89.844\n",
            "EPOCH: 82/100 idx: 350/391 Loss: 0.304 accuracy: 91.406\n",
            "Test accuarcy: 85.26\n",
            "Test average loss: 0.013937409530580043\n",
            "EPOCH: 83/100 idx: 50/391 Loss: 0.345 accuracy: 87.500\n",
            "EPOCH: 83/100 idx: 100/391 Loss: 0.390 accuracy: 89.062\n",
            "EPOCH: 83/100 idx: 150/391 Loss: 0.290 accuracy: 90.625\n",
            "EPOCH: 83/100 idx: 200/391 Loss: 0.239 accuracy: 92.188\n",
            "EPOCH: 83/100 idx: 250/391 Loss: 0.254 accuracy: 92.188\n",
            "EPOCH: 83/100 idx: 300/391 Loss: 0.236 accuracy: 91.406\n",
            "EPOCH: 83/100 idx: 350/391 Loss: 0.402 accuracy: 84.375\n",
            "Test accuarcy: 85.6\n",
            "Test average loss: 0.013807181807234883\n",
            "EPOCH: 84/100 idx: 50/391 Loss: 0.333 accuracy: 89.844\n",
            "EPOCH: 84/100 idx: 100/391 Loss: 0.294 accuracy: 89.844\n",
            "EPOCH: 84/100 idx: 150/391 Loss: 0.421 accuracy: 85.938\n",
            "EPOCH: 84/100 idx: 200/391 Loss: 0.318 accuracy: 89.062\n",
            "EPOCH: 84/100 idx: 250/391 Loss: 0.334 accuracy: 89.062\n",
            "EPOCH: 84/100 idx: 300/391 Loss: 0.302 accuracy: 91.406\n",
            "EPOCH: 84/100 idx: 350/391 Loss: 0.329 accuracy: 86.719\n",
            "Test accuarcy: 86.44\n",
            "Test average loss: 0.012814974243566394\n",
            "Saving\n",
            "EPOCH: 85/100 idx: 50/391 Loss: 0.241 accuracy: 92.188\n",
            "EPOCH: 85/100 idx: 100/391 Loss: 0.243 accuracy: 88.281\n",
            "EPOCH: 85/100 idx: 150/391 Loss: 0.224 accuracy: 89.062\n",
            "EPOCH: 85/100 idx: 200/391 Loss: 0.275 accuracy: 91.406\n",
            "EPOCH: 85/100 idx: 250/391 Loss: 0.253 accuracy: 88.281\n",
            "EPOCH: 85/100 idx: 300/391 Loss: 0.359 accuracy: 85.156\n",
            "EPOCH: 85/100 idx: 350/391 Loss: 0.254 accuracy: 90.625\n",
            "Test accuarcy: 86.56\n",
            "Test average loss: 0.012915834154561162\n",
            "Saving\n",
            "EPOCH: 86/100 idx: 50/391 Loss: 0.184 accuracy: 92.188\n",
            "EPOCH: 86/100 idx: 100/391 Loss: 0.288 accuracy: 89.062\n",
            "EPOCH: 86/100 idx: 150/391 Loss: 0.268 accuracy: 92.188\n",
            "EPOCH: 86/100 idx: 200/391 Loss: 0.345 accuracy: 91.406\n",
            "EPOCH: 86/100 idx: 250/391 Loss: 0.396 accuracy: 88.281\n",
            "EPOCH: 86/100 idx: 300/391 Loss: 0.309 accuracy: 91.406\n",
            "EPOCH: 86/100 idx: 350/391 Loss: 0.276 accuracy: 91.406\n",
            "Test accuarcy: 86.25\n",
            "Test average loss: 0.013214086903631687\n",
            "EPOCH: 87/100 idx: 50/391 Loss: 0.320 accuracy: 90.625\n",
            "EPOCH: 87/100 idx: 100/391 Loss: 0.208 accuracy: 92.969\n",
            "EPOCH: 87/100 idx: 150/391 Loss: 0.327 accuracy: 89.062\n",
            "EPOCH: 87/100 idx: 200/391 Loss: 0.342 accuracy: 87.500\n",
            "EPOCH: 87/100 idx: 250/391 Loss: 0.384 accuracy: 89.844\n",
            "EPOCH: 87/100 idx: 300/391 Loss: 0.154 accuracy: 94.531\n",
            "EPOCH: 87/100 idx: 350/391 Loss: 0.248 accuracy: 90.625\n",
            "Test accuarcy: 87.39\n",
            "Test average loss: 0.011911701625213027\n",
            "Saving\n",
            "EPOCH: 88/100 idx: 50/391 Loss: 0.238 accuracy: 92.188\n",
            "EPOCH: 88/100 idx: 100/391 Loss: 0.188 accuracy: 91.406\n",
            "EPOCH: 88/100 idx: 150/391 Loss: 0.176 accuracy: 95.312\n",
            "EPOCH: 88/100 idx: 200/391 Loss: 0.306 accuracy: 91.406\n",
            "EPOCH: 88/100 idx: 250/391 Loss: 0.411 accuracy: 86.719\n",
            "EPOCH: 88/100 idx: 300/391 Loss: 0.167 accuracy: 93.750\n",
            "EPOCH: 88/100 idx: 350/391 Loss: 0.303 accuracy: 88.281\n",
            "Test accuarcy: 86.48\n",
            "Test average loss: 0.012981513204053044\n",
            "EPOCH: 89/100 idx: 50/391 Loss: 0.262 accuracy: 90.625\n",
            "EPOCH: 89/100 idx: 100/391 Loss: 0.250 accuracy: 92.188\n",
            "EPOCH: 89/100 idx: 150/391 Loss: 0.370 accuracy: 84.375\n",
            "EPOCH: 89/100 idx: 200/391 Loss: 0.381 accuracy: 85.938\n",
            "EPOCH: 89/100 idx: 250/391 Loss: 0.229 accuracy: 90.625\n",
            "EPOCH: 89/100 idx: 300/391 Loss: 0.220 accuracy: 92.969\n",
            "EPOCH: 89/100 idx: 350/391 Loss: 0.416 accuracy: 88.281\n",
            "Test accuarcy: 86.68\n",
            "Test average loss: 0.0124895645160228\n",
            "EPOCH: 90/100 idx: 50/391 Loss: 0.349 accuracy: 88.281\n",
            "EPOCH: 90/100 idx: 100/391 Loss: 0.310 accuracy: 89.844\n",
            "EPOCH: 90/100 idx: 150/391 Loss: 0.186 accuracy: 92.188\n",
            "EPOCH: 90/100 idx: 200/391 Loss: 0.195 accuracy: 93.750\n",
            "EPOCH: 90/100 idx: 250/391 Loss: 0.237 accuracy: 90.625\n",
            "EPOCH: 90/100 idx: 300/391 Loss: 0.268 accuracy: 90.625\n",
            "EPOCH: 90/100 idx: 350/391 Loss: 0.263 accuracy: 92.188\n",
            "Test accuarcy: 86.57\n",
            "Test average loss: 0.012514736155793071\n",
            "EPOCH: 91/100 idx: 50/391 Loss: 0.301 accuracy: 92.969\n",
            "EPOCH: 91/100 idx: 100/391 Loss: 0.224 accuracy: 91.406\n",
            "EPOCH: 91/100 idx: 150/391 Loss: 0.185 accuracy: 94.531\n",
            "EPOCH: 91/100 idx: 200/391 Loss: 0.228 accuracy: 90.625\n",
            "EPOCH: 91/100 idx: 250/391 Loss: 0.251 accuracy: 92.188\n",
            "EPOCH: 91/100 idx: 300/391 Loss: 0.291 accuracy: 86.719\n",
            "EPOCH: 91/100 idx: 350/391 Loss: 0.365 accuracy: 89.844\n",
            "Test accuarcy: 86.96\n",
            "Test average loss: 0.01248499523550272\n",
            "EPOCH: 92/100 idx: 50/391 Loss: 0.306 accuracy: 89.062\n",
            "EPOCH: 92/100 idx: 100/391 Loss: 0.201 accuracy: 92.969\n",
            "EPOCH: 92/100 idx: 150/391 Loss: 0.149 accuracy: 93.750\n",
            "EPOCH: 92/100 idx: 200/391 Loss: 0.194 accuracy: 94.531\n",
            "EPOCH: 92/100 idx: 250/391 Loss: 0.244 accuracy: 89.062\n",
            "EPOCH: 92/100 idx: 300/391 Loss: 0.263 accuracy: 89.844\n",
            "EPOCH: 92/100 idx: 350/391 Loss: 0.363 accuracy: 85.938\n",
            "Test accuarcy: 86.77\n",
            "Test average loss: 0.012698749781399966\n",
            "EPOCH: 93/100 idx: 50/391 Loss: 0.188 accuracy: 94.531\n",
            "EPOCH: 93/100 idx: 100/391 Loss: 0.235 accuracy: 92.969\n",
            "EPOCH: 93/100 idx: 150/391 Loss: 0.331 accuracy: 88.281\n",
            "EPOCH: 93/100 idx: 200/391 Loss: 0.334 accuracy: 89.844\n",
            "EPOCH: 93/100 idx: 250/391 Loss: 0.311 accuracy: 87.500\n",
            "EPOCH: 93/100 idx: 300/391 Loss: 0.147 accuracy: 94.531\n",
            "EPOCH: 93/100 idx: 350/391 Loss: 0.233 accuracy: 91.406\n",
            "Test accuarcy: 86.66\n",
            "Test average loss: 0.013007517201453447\n",
            "EPOCH: 94/100 idx: 50/391 Loss: 0.272 accuracy: 92.188\n",
            "EPOCH: 94/100 idx: 100/391 Loss: 0.242 accuracy: 91.406\n",
            "EPOCH: 94/100 idx: 150/391 Loss: 0.236 accuracy: 90.625\n",
            "EPOCH: 94/100 idx: 200/391 Loss: 0.337 accuracy: 89.844\n",
            "EPOCH: 94/100 idx: 250/391 Loss: 0.338 accuracy: 86.719\n",
            "EPOCH: 94/100 idx: 300/391 Loss: 0.245 accuracy: 89.844\n",
            "EPOCH: 94/100 idx: 350/391 Loss: 0.379 accuracy: 86.719\n",
            "Test accuarcy: 86.63\n",
            "Test average loss: 0.013051193810999394\n",
            "EPOCH: 95/100 idx: 50/391 Loss: 0.173 accuracy: 93.750\n",
            "EPOCH: 95/100 idx: 100/391 Loss: 0.397 accuracy: 85.938\n",
            "EPOCH: 95/100 idx: 150/391 Loss: 0.208 accuracy: 95.312\n",
            "EPOCH: 95/100 idx: 200/391 Loss: 0.291 accuracy: 89.062\n",
            "EPOCH: 95/100 idx: 250/391 Loss: 0.236 accuracy: 91.406\n",
            "EPOCH: 95/100 idx: 300/391 Loss: 0.157 accuracy: 95.312\n",
            "EPOCH: 95/100 idx: 350/391 Loss: 0.231 accuracy: 89.844\n",
            "Test accuarcy: 86.94\n",
            "Test average loss: 0.0126684920668602\n",
            "EPOCH: 96/100 idx: 50/391 Loss: 0.308 accuracy: 89.062\n",
            "EPOCH: 96/100 idx: 100/391 Loss: 0.212 accuracy: 92.969\n",
            "EPOCH: 96/100 idx: 150/391 Loss: 0.140 accuracy: 96.094\n",
            "EPOCH: 96/100 idx: 200/391 Loss: 0.284 accuracy: 91.406\n",
            "EPOCH: 96/100 idx: 250/391 Loss: 0.262 accuracy: 91.406\n",
            "EPOCH: 96/100 idx: 300/391 Loss: 0.198 accuracy: 93.750\n",
            "EPOCH: 96/100 idx: 350/391 Loss: 0.317 accuracy: 86.719\n",
            "Test accuarcy: 86.84\n",
            "Test average loss: 0.012763355511426926\n",
            "EPOCH: 97/100 idx: 50/391 Loss: 0.280 accuracy: 92.969\n",
            "EPOCH: 97/100 idx: 100/391 Loss: 0.221 accuracy: 92.969\n",
            "EPOCH: 97/100 idx: 150/391 Loss: 0.235 accuracy: 92.969\n",
            "EPOCH: 97/100 idx: 200/391 Loss: 0.328 accuracy: 88.281\n",
            "EPOCH: 97/100 idx: 250/391 Loss: 0.232 accuracy: 92.969\n",
            "EPOCH: 97/100 idx: 300/391 Loss: 0.233 accuracy: 94.531\n",
            "EPOCH: 97/100 idx: 350/391 Loss: 0.207 accuracy: 93.750\n",
            "Test accuarcy: 87.33\n",
            "Test average loss: 0.012261653674021364\n",
            "EPOCH: 98/100 idx: 50/391 Loss: 0.175 accuracy: 93.750\n",
            "EPOCH: 98/100 idx: 100/391 Loss: 0.285 accuracy: 89.844\n",
            "EPOCH: 98/100 idx: 150/391 Loss: 0.212 accuracy: 89.844\n",
            "EPOCH: 98/100 idx: 200/391 Loss: 0.175 accuracy: 94.531\n",
            "EPOCH: 98/100 idx: 250/391 Loss: 0.220 accuracy: 94.531\n",
            "EPOCH: 98/100 idx: 300/391 Loss: 0.257 accuracy: 91.406\n",
            "EPOCH: 98/100 idx: 350/391 Loss: 0.210 accuracy: 93.750\n",
            "Test accuarcy: 86.92\n",
            "Test average loss: 0.012395842637866736\n",
            "EPOCH: 99/100 idx: 50/391 Loss: 0.279 accuracy: 89.844\n",
            "EPOCH: 99/100 idx: 100/391 Loss: 0.305 accuracy: 89.844\n",
            "EPOCH: 99/100 idx: 150/391 Loss: 0.246 accuracy: 90.625\n",
            "EPOCH: 99/100 idx: 200/391 Loss: 0.197 accuracy: 92.188\n",
            "EPOCH: 99/100 idx: 250/391 Loss: 0.216 accuracy: 91.406\n",
            "EPOCH: 99/100 idx: 300/391 Loss: 0.219 accuracy: 92.188\n",
            "EPOCH: 99/100 idx: 350/391 Loss: 0.462 accuracy: 87.500\n",
            "Test accuarcy: 86.78\n",
            "Test average loss: 0.013005283166468144\n",
            "EPOCH: 100/100 idx: 50/391 Loss: 0.198 accuracy: 94.531\n",
            "EPOCH: 100/100 idx: 100/391 Loss: 0.232 accuracy: 89.062\n",
            "EPOCH: 100/100 idx: 150/391 Loss: 0.290 accuracy: 89.062\n",
            "EPOCH: 100/100 idx: 200/391 Loss: 0.220 accuracy: 94.531\n",
            "EPOCH: 100/100 idx: 250/391 Loss: 0.247 accuracy: 88.281\n",
            "EPOCH: 100/100 idx: 300/391 Loss: 0.306 accuracy: 89.062\n",
            "EPOCH: 100/100 idx: 350/391 Loss: 0.287 accuracy: 90.625\n",
            "Test accuarcy: 87.29\n",
            "Test average loss: 0.012342796632647514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ovXbJEhHf7Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "2e36c01f-1358-41f2-97a7-ad02808f6ac4"
      },
      "source": [
        "while True:\r\n",
        "    continue"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6a65cf439648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgSzlAeIHGm7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}